{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCccHb2Rxh9pBDq8hyo/3v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MineCamci/My_Project_For_Data_Science/blob/main/Salary_Prediction_(Feature_Engineering).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdmnJ1SHbFi0"
      },
      "outputs": [],
      "source": [
        "############################################\n",
        "# Gerekli Kütüphane ve Fonksiyonlar\n",
        "############################################\n",
        "\n",
        "import numpy as np\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate, cross_val_score, validation_curve\n",
        "\n",
        "from catboost import CatBoostRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 170)\n",
        "pd.set_option('display.max_rows', 20)\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action=\"ignore\", category=ConvergenceWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
        "\n",
        "\n",
        "#############################################\n",
        "# GELİŞMİŞ FONKSİYONEL KEŞİFÇİ VERİ ANALİZİ (ADVANCED FUNCTIONAL EDA)\n",
        "#############################################\n",
        "\n",
        "# 1. Genel Resim\n",
        "# 2. Kategorik Değişken Analizi (Analysis of Categorical Variables)\n",
        "# 3. Sayısal Değişken Analizi (Analysis of Numerical Variables)\n",
        "# 4. Hedef Değişken Analizi (Analysis of Target Variable)\n",
        "# 5. Korelasyon Analizi (Analysis of Correlation)\n",
        "\n",
        "\n",
        "#############################################\n",
        "# 1. Genel Resim\n",
        "#############################################\n",
        "\n",
        "df = pd.read_csv(\"7Week_ML_Part1/salary_prediction/Hitters.csv\")\n",
        "\n",
        "def check_df(dataframe, head=5):\n",
        "    print(\"##################### Shape #####################\")\n",
        "    print(dataframe.shape)\n",
        "\n",
        "    print(\"##################### Types #####################\")\n",
        "    print(dataframe.dtypes)\n",
        "\n",
        "    print(\"##################### Head #####################\")\n",
        "    print(dataframe.head(head))\n",
        "\n",
        "    #print(\"##################### Tail #####################\")\n",
        "    #print(dataframe.tail(head))\n",
        "\n",
        "    print(\"##################### NA #####################\")\n",
        "    print(dataframe.isnull().sum())\n",
        "\n",
        "    print(\"##################### Quantiles #####################\")\n",
        "    print(dataframe.describe([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n",
        "\n",
        "check_df(df)\n",
        "\n",
        "\n",
        "def grab_col_names(dataframe, cat_th=10, car_th=20):\n",
        "    \"\"\"\n",
        "    Veri setindeki kategorik, numerik ve kategorik fakat kardinal değişkenlerin isimlerini verir.\n",
        "    Not: Kategorik değişkenlerin içerisine numerik görünümlü kategorik değişkenler de dahildir.\n",
        "\n",
        "    Parameters\n",
        "    ------\n",
        "        dataframe: dataframe\n",
        "                Değişken isimleri alınmak istenilen dataframe\n",
        "        cat_th: int, optional\n",
        "                numerik fakat kategorik olan değişkenler için sınıf eşik değeri\n",
        "        car_th: int, optinal\n",
        "                kategorik fakat kardinal değişkenler için sınıf eşik değeri\n",
        "\n",
        "    Returns\n",
        "    ------\n",
        "        cat_cols: list\n",
        "                Kategorik değişken listesi\n",
        "        num_cols: list\n",
        "                Numerik değişken listesi\n",
        "        cat_but_car: list\n",
        "                Kategorik görünümlü kardinal değişken listesi\n",
        "\n",
        "    Examples\n",
        "    ------\n",
        "        import seaborn as sns\n",
        "        df = sns.load_dataset(\"iris\")\n",
        "        print(grab_col_names(df))\n",
        "\n",
        "\n",
        "    Notes\n",
        "    ------\n",
        "        cat_cols + num_cols + cat_but_car = toplam değişken sayısı\n",
        "        num_but_cat cat_cols'un içerisinde.\n",
        "        Return olan 3 liste toplamı toplam değişken sayısına eşittir: cat_cols + num_cols + cat_but_car = değişken sayısı\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # cat_cols, cat_but_car\n",
        "    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n",
        "    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n",
        "                   dataframe[col].dtypes != \"O\"]\n",
        "    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n",
        "                   dataframe[col].dtypes == \"O\"]\n",
        "    cat_cols = cat_cols + num_but_cat\n",
        "    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n",
        "\n",
        "    # num_cols\n",
        "    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n",
        "    num_cols = [col for col in num_cols if col not in num_but_cat]\n",
        "\n",
        "    print(f\"Observations: {dataframe.shape[0]}\")\n",
        "    print(f\"Variables: {dataframe.shape[1]}\")\n",
        "    print(f'cat_cols: {len(cat_cols)}')\n",
        "    print(f'num_cols: {len(num_cols)}')\n",
        "    print(f'cat_but_car: {len(cat_but_car)}')\n",
        "    print(f'num_but_cat: {len(num_but_cat)}')\n",
        "    return cat_cols, num_cols, cat_but_car\n",
        "\n",
        "cat_cols, num_cols, cat_but_car = grab_col_names(df)\n",
        "\n",
        "\n",
        "#############################################\n",
        "# 2. Kategorik Değişken Analizi (Analysis of Categorical Variables)\n",
        "#############################################\n",
        "\n",
        "def cat_summary(dataframe, col_name, plot=False):\n",
        "\n",
        "    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n",
        "                        \"Ratio\": 100 * dataframe[col_name].value_counts() / len(dataframe)}))\n",
        "    print(\"##########################################\")\n",
        "\n",
        "    if plot:\n",
        "        sns.countplot(x=dataframe[col_name], data=dataframe)\n",
        "        plt.show(block=True)\n",
        "\n",
        "for col in cat_cols:\n",
        "    cat_summary(df, col, plot=True)\n",
        "\n",
        "\n",
        "#############################################\n",
        "# 3. Sayısal Değişken Analizi (Analysis of Numerical Variables)\n",
        "#############################################\n",
        "\n",
        "def num_summary(dataframe, numerical_col, plot=False):\n",
        "\n",
        "    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n",
        "\n",
        "    print(dataframe[numerical_col].describe(quantiles).T)\n",
        "\n",
        "    if plot:\n",
        "        dataframe[numerical_col].hist(bins=20)\n",
        "        plt.xlabel(numerical_col)\n",
        "        plt.title(numerical_col)\n",
        "        plt.show(block=True)\n",
        "\n",
        "for col in num_cols:\n",
        "    num_summary(df, col, plot=True)\n",
        "\n",
        "\n",
        "#############################################\n",
        "# 4. Hedef Değişken Analizi (Analysis of Target Variable)\n",
        "#############################################\n",
        "\n",
        "def target_summary_with_cat(dataframe, target, categorical_col):\n",
        "\n",
        "    print(pd.DataFrame({\"TARGET_MEAN\": dataframe.groupby(categorical_col)[target].mean()}), end=\"\\n\\n\\n\")\n",
        "\n",
        "for col in cat_cols:\n",
        "    target_summary_with_cat(df, \"Salary\", col)\n",
        "\n",
        "\n",
        "#############################################\n",
        "# 5. Korelasyon Analizi (Analysis of Correlation)\n",
        "#############################################\n",
        "\n",
        "df[num_cols].corr(method=\"spearman\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(25,10))\n",
        "sns.heatmap(df[num_cols].corr(), annot=True, linewidths=.5, ax=ax)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# correlation with the final state of the variables\n",
        "plt.figure(figsize=(45,45))\n",
        "corr=df[num_cols].corr()\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "sns.heatmap(df[num_cols].corr(), mask=mask, cmap='coolwarm', vmax=.3, center=0,\n",
        "            square=True, linewidths=.5,annot=True)\n",
        "plt.show(block=True)\n",
        "\n",
        "# kategorik degisken ordinal\n",
        "# map - ortaokul -0 , lisans 1 , yükseklisans 2,\n",
        "\n",
        "\n",
        "def find_correlation(dataframe, numeric_cols, corr_limit=0.60):\n",
        "    high_correlations = []\n",
        "    low_correlations = []\n",
        "    for col in numeric_cols:\n",
        "        if col == \"Salary\":\n",
        "            pass\n",
        "        else:\n",
        "            correlation = dataframe[[col, \"Salary\"]].corr().loc[col, \"Salary\"]\n",
        "            print(col, correlation)\n",
        "            if abs(correlation) > corr_limit:\n",
        "                high_correlations.append(col + \": \" + str(correlation))\n",
        "            else:\n",
        "                low_correlations.append(col + \": \" + str(correlation))\n",
        "    return low_correlations, high_correlations\n",
        "\n",
        "\n",
        "low_corrs, high_corrs = find_correlation(df, num_cols)\n",
        "\n",
        "\n",
        "\n",
        "#############################################\n",
        "# GELİŞMİŞ FONKSİYONEL KEŞİFÇİ VERİ ANALİZİ (ADVANCED FUNCTIONAL EDA)\n",
        "#############################################\n",
        "\n",
        "# 1. Outliers (Aykırı Değerler)\n",
        "# 2. Missing Values (Eksik Değerler)\n",
        "# 3. Feature Extraction (Özellik Çıkarımı)\n",
        "# 4. Encoding (Label Encoding, One-Hot Encoding, Rare Encoding)\n",
        "# 5. Feature Scaling (Özellik Ölçeklendirme)\n",
        "\n",
        "\n",
        "#############################################\n",
        "# 1. Outliers (Aykırı Değerler)\n",
        "#############################################\n",
        "\n",
        "\n",
        "sns.boxplot(x=df[\"Salary\"], data=df)\n",
        "plt.show(block=True)\n",
        "\n",
        "\n",
        "def outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):\n",
        "    quartile1 = dataframe[col_name].quantile(q1)\n",
        "    quartile3 = dataframe[col_name].quantile(q3)\n",
        "    interquantile_range = quartile3 - quartile1\n",
        "    up_limit = quartile3 + 1.5 * interquantile_range\n",
        "    low_limit = quartile1 - 1.5 * interquantile_range\n",
        "    return low_limit, up_limit\n",
        "\n",
        "def check_outlier(dataframe, col_name):\n",
        "    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n",
        "    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "def replace_with_thresholds(dataframe, variable):\n",
        "    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n",
        "    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n",
        "    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit\n",
        "\n",
        "for col in num_cols:\n",
        "    print(col, check_outlier(df, col))\n",
        "\n",
        "for col in num_cols:\n",
        "    if check_outlier(df, col):\n",
        "        replace_with_thresholds(df, col)\n",
        "\n",
        "\n",
        "#############################################\n",
        "# 2. Missing Values (Eksik Değerler)\n",
        "#############################################\n",
        "\n",
        "def missing_values_table(dataframe, na_name=False):\n",
        "    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n",
        "    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n",
        "    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n",
        "    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n",
        "    print(missing_df, end=\"\\n\")\n",
        "    if na_name:\n",
        "        return na_columns\n",
        "\n",
        "missing_values_table(df)\n",
        "\n",
        "\n",
        "# Eksik veri analizine uygun olarak 3 farkli yöntem kullanabiliriz.\n",
        "df1 = df.copy()\n",
        "df1.head()\n",
        "cat_cols, num_cols, cat_but_car = grab_col_names(df1)\n",
        "\n",
        "\n",
        "method = int(input(\"Eksik veri için hangi yöntemi uygulamak istersiniz? (1/2/3): \"))\n",
        "\n",
        "if method ==1:\n",
        "    dff = pd.get_dummies(df1[cat_cols + num_cols], drop_first=True)\n",
        "    scaler = RobustScaler()\n",
        "    dff = pd.DataFrame(scaler.fit_transform(dff), columns=dff.columns)\n",
        "    imputer = KNNImputer(n_neighbors=5)\n",
        "    dff = pd.DataFrame(imputer.fit_transform(dff), columns=dff.columns)\n",
        "    dff = pd.DataFrame(scaler.inverse_transform(dff), columns=dff.columns)\n",
        "    df1 = dff\n",
        "\n",
        "\n",
        "elif method ==2:\n",
        "    df1.loc[(df1[\"Salary\"].isnull()) & (df1[\"League\"] == \"A\") & (df[\"Division\"] == \"E\"), \"Salary\"] = \\\n",
        "    df1.groupby([\"League\", \"Division\"])[\"Salary\"].mean()[\"A\", \"E\"]\n",
        "\n",
        "    df1.loc[(df1[\"Salary\"].isnull()) & (df1[\"League\"] == \"A\") & (df[\"Division\"] == \"W\"), \"Salary\"] = \\\n",
        "    df1.groupby([\"League\", \"Division\"])[\"Salary\"].mean()[\"A\", \"W\"]\n",
        "\n",
        "    df1.loc[(df1[\"Salary\"].isnull()) & (df1[\"League\"] == \"N\") & (df[\"Division\"] == \"E\"), \"Salary\"] = \\\n",
        "    df1.groupby([\"League\", \"Division\"])[\"Salary\"].mean()[\"N\", \"E\"]\n",
        "\n",
        "    df1.loc[(df1[\"Salary\"].isnull()) & (df1[\"League\"] == \"N\") & (df[\"Division\"] == \"W\"), \"Salary\"] = \\\n",
        "    df1.groupby([\"League\", \"Division\"])[\"Salary\"].mean()[\"N\", \"W\"]\n",
        "\n",
        "\n",
        "elif method == 3:\n",
        "    # Drop NA\n",
        "    # Eksik değer içeren tüm satırları silme\n",
        "    df1.dropna(inplace=True)\n",
        "\n",
        "print(df1.head())\n",
        "print(df1.isnull().sum())\n",
        "\n",
        "def eksik_veri_doldur(dataframe,method):\n",
        "    df1 = dataframe.copy()\n",
        "    cat_cols, num_cols, cat_but_car = grab_col_names(df1)\n",
        "    if method == 1:\n",
        "        dff = pd.get_dummies(df1[cat_cols + num_cols], drop_first=True)\n",
        "        scaler = RobustScaler()\n",
        "        dff = pd.DataFrame(scaler.fit_transform(dff), columns=dff.columns)\n",
        "        imputer = KNNImputer(n_neighbors=5)\n",
        "        dff = pd.DataFrame(imputer.fit_transform(dff), columns=dff.columns)\n",
        "        dff = pd.DataFrame(scaler.inverse_transform(dff), columns=dff.columns)\n",
        "        df1 = dff\n",
        "        pass\n",
        "\n",
        "\n",
        "    elif method == 2:\n",
        "        df1.loc[(df1[\"Salary\"].isnull()) & (df1[\"League\"] == \"A\") & (df[\"Division\"] == \"E\"), \"Salary\"] = \\\n",
        "            df1.groupby([\"League\", \"Division\"])[\"Salary\"].mean()[\"A\", \"E\"]\n",
        "\n",
        "        df1.loc[(df1[\"Salary\"].isnull()) & (df1[\"League\"] == \"A\") & (df[\"Division\"] == \"W\"), \"Salary\"] = \\\n",
        "            df1.groupby([\"League\", \"Division\"])[\"Salary\"].mean()[\"A\", \"W\"]\n",
        "\n",
        "        df1.loc[(df1[\"Salary\"].isnull()) & (df1[\"League\"] == \"N\") & (df[\"Division\"] == \"E\"), \"Salary\"] = \\\n",
        "            df1.groupby([\"League\", \"Division\"])[\"Salary\"].mean()[\"N\", \"E\"]\n",
        "\n",
        "        df1.loc[(df1[\"Salary\"].isnull()) & (df1[\"League\"] == \"N\") & (df[\"Division\"] == \"W\"), \"Salary\"] = \\\n",
        "            df1.groupby([\"League\", \"Division\"])[\"Salary\"].mean()[\"N\", \"W\"]\n",
        "        pass\n",
        "\n",
        "    elif method == 3:\n",
        "        # Drop NA\n",
        "        # Eksik değer içeren tüm satırları silme\n",
        "        df1.dropna(inplace=True)\n",
        "        pass\n",
        "    return df1\n",
        "\n",
        "df1 = eksik_veri_doldur(df,method=1)\n",
        "\n",
        "#############################################\n",
        "# 3. Feature Extraction (Özellik Çıkarımı)\n",
        "#############################################\n",
        "\n",
        "new_num_cols=[col for col in num_cols if col!=\"Salary\"]\n",
        "df1[new_num_cols]=df1[new_num_cols]+0.0000000001\n",
        "\n",
        "df1[\"Hits_Success\"] = (df1[\"Hits\"] / df1[\"AtBat\"]) * 100\n",
        "df1['NEW_RBI'] = df1['RBI'] / df1['CRBI']\n",
        "df1['NEW_Walks'] = df1['Walks'] / df1['CWalks']\n",
        "df1['NEW_PutOuts'] = df1['PutOuts'] * df1['Years']\n",
        "df1['NEW_Hits'] = df1['Hits'] / df1['CHits'] + df1['Hits']\n",
        "df1[\"NEW_CRBI*CATBAT\"] = df1['CRBI'] * df1['CAtBat']\n",
        "df1[\"NEW_Chits\"] = df1[\"CHits\"] / df1[\"Years\"]\n",
        "df1[\"NEW_CHmRun\"] = df1[\"CHmRun\"] * df1[\"Years\"]\n",
        "df1[\"NEW_CRuns\"] = df1[\"CRuns\"] / df1[\"Years\"]\n",
        "df1[\"NEW_Chits\"] = df1[\"CHits\"] * df1[\"Years\"]\n",
        "df1[\"NEW_RW\"] = df1[\"RBI\"] * df1[\"Walks\"]\n",
        "df1[\"NEW_RBWALK\"] = df1[\"RBI\"] / df1[\"Walks\"]\n",
        "df1[\"NEW_CH_CB\"] = df1[\"CHits\"] / df1[\"CAtBat\"]\n",
        "df1[\"NEW_CHm_CAT\"] = df1[\"CHmRun\"] / df1[\"CAtBat\"]\n",
        "df1['NEW_Diff_Atbat'] = df1['AtBat'] - (df1['CAtBat'] / df1['Years'])\n",
        "df1['NEW_Diff_Hits'] = df1['Hits'] - (df1['CHits'] / df1['Years'])\n",
        "df1['NEW_Diff_HmRun'] = df1['HmRun'] - (df1['CHmRun'] / df1['Years'])\n",
        "df1['NEW_Diff_Runs'] = df1['Runs'] - (df1['CRuns'] / df1['Years'])\n",
        "df1['NEW_Diff_RBI'] = df1['RBI'] - (df1['CRBI'] / df1['Years'])\n",
        "df1['NEW_Diff_Walks'] = df1['Walks'] - (df1['CWalks'] / df1['Years'])\n",
        "\n",
        "def feature_ext(df1):\n",
        "    cat_cols, num_cols, cat_but_car = grab_col_names(df1)\n",
        "    new_num_cols = [col for col in num_cols if col != \"Salary\"]\n",
        "    df1[new_num_cols] = df1[new_num_cols] + 0.0000000001\n",
        "\n",
        "    df1['NEW_Hits'] = df1['Hits'] / df1['CHits'] + df1['Hits']\n",
        "    df1['NEW_RBI'] = df1['RBI'] / df1['CRBI']\n",
        "    df1['NEW_Walks'] = df1['Walks'] / df1['CWalks']\n",
        "    df1['NEW_PutOuts'] = df1['PutOuts'] * df1['Years']\n",
        "    df1[\"Hits_Success\"] = (df1[\"Hits\"] / df1[\"AtBat\"]) * 100\n",
        "    df1[\"NEW_CRBI*CATBAT\"] = df1['CRBI'] * df1['CAtBat']\n",
        "    df1[\"NEW_RBI\"] = df1[\"RBI\"] / df1[\"CRBI\"]\n",
        "    df1[\"NEW_Chits\"] = df1[\"CHits\"] / df1[\"Years\"]\n",
        "    df1[\"NEW_CHmRun\"] = df1[\"CHmRun\"] * df1[\"Years\"]\n",
        "    df1[\"NEW_CRuns\"] = df1[\"CRuns\"] / df1[\"Years\"]\n",
        "    df1[\"NEW_Chits\"] = df1[\"CHits\"] * df1[\"Years\"]\n",
        "    df1[\"NEW_RW\"] = df1[\"RBI\"] * df1[\"Walks\"]\n",
        "    df1[\"NEW_RBWALK\"] = df1[\"RBI\"] / df1[\"Walks\"]\n",
        "    df1[\"NEW_CH_CB\"] = df1[\"CHits\"] / df1[\"CAtBat\"]\n",
        "    df1[\"NEW_CHm_CAT\"] = df1[\"CHmRun\"] / df1[\"CAtBat\"]\n",
        "    df1['NEW_Diff_Atbat'] = df1['AtBat'] - (df1['CAtBat'] / df1['Years'])\n",
        "    df1['NEW_Diff_Hits'] = df1['Hits'] - (df1['CHits'] / df1['Years'])\n",
        "    df1['NEW_Diff_HmRun'] = df1['HmRun'] - (df1['CHmRun'] / df1['Years'])\n",
        "    df1['NEW_Diff_Runs'] = df1['Runs'] - (df1['CRuns'] / df1['Years'])\n",
        "    df1['NEW_Diff_RBI'] = df1['RBI'] - (df1['CRBI'] / df1['Years'])\n",
        "    df1['NEW_Diff_Walks'] = df1['Walks'] - (df1['CWalks'] / df1['Years'])\n",
        "    return df1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#############################################\n",
        "# 4. One-Hot Encoding\n",
        "#############################################\n",
        "\n",
        "cat_cols, num_cols, cat_but_car = grab_col_names(df1)\n",
        "\n",
        "def one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n",
        "    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n",
        "    for col in dataframe.columns:\n",
        "        if dataframe[col].dtype == 'bool':\n",
        "            dataframe[col] = dataframe[col].astype(int)\n",
        "    return dataframe\n",
        "\n",
        "df1 = one_hot_encoder(df1, cat_cols, drop_first=True)\n",
        "\n",
        "\n",
        "#############################################\n",
        "# 5. Feature Scaling (Özellik Ölçeklendirme)\n",
        "#############################################\n",
        "\n",
        "cat_cols, num_cols, cat_but_car = grab_col_names(df1)\n",
        "\n",
        "num_cols = [col for col in num_cols if col not in [\"Salary\"]]\n",
        "scaler = StandardScaler()\n",
        "df1[num_cols] = scaler.fit_transform(df1[num_cols])\n",
        "df1.head()\n",
        "\n",
        "\n",
        "def feature_scaling(dataframe, num_cols):\n",
        "    # Özellik ölçeklendirme işlemleri\n",
        "    scaler = StandardScaler()\n",
        "    cat_cols, num_cols, cat_but_car = grab_col_names(dataframe)\n",
        "    num_cols = [col for col in num_cols if col not in [\"Salary\"]]\n",
        "    dataframe[num_cols] = scaler.fit_transform(dataframe[num_cols])\n",
        "    return dataframe\n",
        "\n",
        "# Correlation Analysis\n",
        "fig, ax = plt.subplots(figsize=(25,10))\n",
        "sns.heatmap(df1.corr(), annot=True, linewidths=.5, ax=ax)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#############################################\n",
        "#               MODELING                    #\n",
        "#############################################\n",
        "df1.isnull().sum().sum()\n",
        "#df1.dropna(inplace=True)\n",
        "y = df1[\"Salary\"]\n",
        "X = df1.drop([\"Salary\"], axis=1)\n",
        "\n",
        "X.shape\n",
        "y.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.20,\n",
        "                                                    random_state=46)\n",
        "\n",
        "# Model Evaluation for Linear Regression\n",
        "linreg = LinearRegression()\n",
        "model = linreg.fit(X_train,y_train)\n",
        "y_pred = model.predict(X_train)\n",
        "lin_train_rmse =np.sqrt(mean_squared_error(y_train,y_pred))\n",
        "print(\"LINEAR REGRESSION TRAIN RMSE:\", \"{:,.2f}\".format(np.sqrt(mean_squared_error(y_train,y_pred))))\n",
        "\n",
        "lin_train_r2 = linreg.score(X_train,y_train)\n",
        "print(\"LINEAR REGRESSION TRAIN R-SQUARED:\", \"{:,.3f}\".format(linreg.score(X_train,y_train)))\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "lin_test_rmse =np.sqrt(mean_squared_error(y_test,y_pred))\n",
        "print(\"LINEAR REGRESSION TEST RMSE:\", \"{:,.2f}\".format(np.sqrt(mean_squared_error(y_test,y_pred))))\n",
        "\n",
        "lin_test_r2 = linreg.score(X_test,y_test)\n",
        "print(\"LINEAR REGRESSION TEST R-SQUARED:\", \"{:,.3f}\".format(linreg.score(X_test,y_test)))\n",
        "\n",
        "\n",
        "# Test part regplot:\n",
        "g = sns.regplot(x=y_test, y=y_pred, scatter_kws={'color': 'b', 's': 5},\n",
        "                ci=False, color=\"r\")\n",
        "g.set_title(f\"Test Model R2: = {linreg.score(X_test, y_test):.3f}\")\n",
        "g.set_ylabel(\"Predicted Salary\")\n",
        "g.set_xlabel(\"Salary\")\n",
        "plt.xlim(-5, 2700)\n",
        "plt.ylim(bottom=0)\n",
        "plt.show(block=True)\n",
        "\n",
        "\n",
        "# Cross Validation Score\n",
        "print(\"LINEAR REGRESSION CROSS_VAL_SCORE:\", \"{:,.3f}\".format(np.mean(np.sqrt(-cross_val_score(model,\n",
        "                                 X,\n",
        "                                 y,\n",
        "                                 cv=10,\n",
        "                                 scoring=\"neg_mean_squared_error\")))))\n",
        "\n",
        "# Bagimsiz degiskenin bagimli degiskene etkisini görebiliyoruz?\n",
        "#\n",
        "\n",
        "# OLS for Linear Regression\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.formula.api import ols\n",
        "\n",
        "# Adding a constant to the model (necessary for statsmodels)\n",
        "X_train_sm = sm.add_constant(X_train)\n",
        "\n",
        "# Fitting the model using statsmodels\n",
        "model_sm = sm.OLS(y_train, X_train_sm).fit()\n",
        "\n",
        "# Getting the summary of the regression model\n",
        "model_summary = model_sm.summary()\n",
        "model_summary\n",
        "\n",
        "def model_training(dataframe,target_col):\n",
        "    y = dataframe[target_col]\n",
        "    X = dataframe.drop([target_col], axis=1)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    test_size=0.20,\n",
        "                                                    random_state=46)\n",
        "\n",
        "    # Model Evaluation for Linear Regression\n",
        "    linreg = LinearRegression()\n",
        "    model = linreg.fit(X_train,y_train)\n",
        "    y_pred = model.predict(X_train)\n",
        "    lin_train_rmse =np.sqrt(mean_squared_error(y_train,y_pred))\n",
        "    print(\"LINEAR REGRESSION TRAIN RMSE:\", \"{:,.2f}\".format(np.sqrt(mean_squared_error(y_train,y_pred))))\n",
        "\n",
        "    lin_train_r2 = linreg.score(X_train,y_train)\n",
        "    print(\"LINEAR REGRESSION TRAIN R-SQUARED:\", \"{:,.3f}\".format(linreg.score(X_train,y_train)))\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    lin_test_rmse =np.sqrt(mean_squared_error(y_test,y_pred))\n",
        "    print(\"LINEAR REGRESSION TEST RMSE:\", \"{:,.2f}\".format(np.sqrt(mean_squared_error(y_test,y_pred))))\n",
        "\n",
        "    lin_test_r2 = linreg.score(X_test,y_test)\n",
        "    print(\"LINEAR REGRESSION TEST R-SQUARED:\", \"{:,.3f}\".format(linreg.score(X_test,y_test)))\n",
        "    # Adding a constant to the model (necessary for statsmodels)\n",
        "    #X_train_sm = sm.add_constant(X_train)\n",
        "    # Fitting the model using statsmodels\n",
        "    #model_sm = sm.OLS(y_train, X_train_sm).fit()\n",
        "    # Getting the summary of the regression model\n",
        "    #model_summary = model_sm.summary()\n",
        "    #return model_summary\n",
        "\n",
        "\n",
        "##### Functions ######\n",
        "\n",
        "def sonuc(df,method):\n",
        "    cat_cols, num_cols, cat_but_car = grab_col_names(df)\n",
        "    df1 = eksik_veri_doldur(df,method)\n",
        "    df1 = feature_ext(df1)\n",
        "    cat_cols, num_cols, cat_but_car = grab_col_names(df1)\n",
        "    df1 = one_hot_encoder(df1, cat_cols, drop_first=True)\n",
        "    df1 = feature_scaling(df1, num_cols)\n",
        "    model = model_training(df1,\"Salary\")\n",
        "\n",
        "sonuc(df,method=1)\n",
        "\n",
        "#############################################\n",
        "# All Models\n",
        "#############################################\n",
        "# Gelismis Agac Yöntemleri sonrasinda bu alana devam edebilirsiniz.\n",
        "# Keyifli calismalar...\n",
        "\n",
        "y = df[\"Salary\"]\n",
        "X = df.drop([\"Salary\"], axis=1)\n",
        "\n",
        "models = [('LR', LinearRegression()),\n",
        "          (\"Ridge\", Ridge()),\n",
        "          (\"Lasso\", Lasso()),\n",
        "          (\"ElasticNet\", ElasticNet()),\n",
        "          ('KNN', KNeighborsRegressor()),\n",
        "          ('CART', DecisionTreeRegressor()),\n",
        "          ('RF', RandomForestRegressor()),\n",
        "          ('SVR', SVR()),\n",
        "          ('GBM', GradientBoostingRegressor()),\n",
        "          (\"XGBoost\", XGBRegressor(objective='reg:squarederror')),\n",
        "          (\"LightGBM\", LGBMRegressor(verbose=-1)),\n",
        "          (\"CatBoost\", CatBoostRegressor(verbose=False))]\n",
        "\n",
        "\n",
        "for name, regressor in models:\n",
        "    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n",
        "    print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n",
        "\n",
        "\n",
        "\n",
        "################################################\n",
        "# Random Forests\n",
        "################################################\n",
        "\n",
        "rf_model = RandomForestRegressor(random_state=17)\n",
        "\n",
        "rf_params = {\"max_depth\": [5, 8, 15, None],\n",
        "             \"max_features\": [5, 7, \"auto\"],\n",
        "             \"min_samples_split\": [8, 15, 20],\n",
        "             \"n_estimators\": [200, 500]}\n",
        "\n",
        "rf_best_grid = GridSearchCV(rf_model, rf_params, cv=5, n_jobs=-1, verbose=True).fit(X, y)\n",
        "rf_final = rf_model.set_params(**rf_best_grid.best_params_, random_state=17).fit(X, y)\n",
        "rmse = np.mean(np.sqrt(-cross_val_score(rf_final, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n",
        "rmse\n",
        "\n",
        "################################################\n",
        "# GBM Model\n",
        "################################################\n",
        "\n",
        "gbm_model = GradientBoostingRegressor(random_state=17)\n",
        "\n",
        "gbm_params = {\"learning_rate\": [0.01, 0.1],\n",
        "              \"max_depth\": [3, 8],\n",
        "              \"n_estimators\": [500, 1000],\n",
        "              \"subsample\": [1, 0.5, 0.7]}\n",
        "\n",
        "gbm_best_grid = GridSearchCV(gbm_model, gbm_params, cv=5, n_jobs=-1, verbose=True).fit(X, y)\n",
        "gbm_final = gbm_model.set_params(**gbm_best_grid.best_params_, random_state=17, ).fit(X, y)\n",
        "rmse = np.mean(np.sqrt(-cross_val_score(gbm_final, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n",
        "rmse\n",
        "\n",
        "################################################\n",
        "# LightGBM\n",
        "################################################\n",
        "\n",
        "lgbm_model = LGBMRegressor(random_state=17)\n",
        "\n",
        "lgbm_params = {\"learning_rate\": [0.01, 0.1],\n",
        "                \"n_estimators\": [300, 500],\n",
        "                \"colsample_bytree\": [0.7, 1]}\n",
        "\n",
        "lgbm_best_grid = GridSearchCV(lgbm_model, lgbm_params, cv=5, n_jobs=-1, verbose=True).fit(X, y)\n",
        "lgbm_final = lgbm_model.set_params(**lgbm_best_grid.best_params_, random_state=17).fit(X, y)\n",
        "rmse = np.mean(np.sqrt(-cross_val_score(lgbm_final, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n",
        "rmse\n",
        "\n",
        "################################################\n",
        "# CatBoost\n",
        "################################################\n",
        "\n",
        "catboost_model = CatBoostRegressor(random_state=17, verbose=False)\n",
        "\n",
        "catboost_params = {\"iterations\": [200, 500],\n",
        "                   \"learning_rate\": [0.01, 0.1],\n",
        "                   \"depth\": [3, 6]}\n",
        "\n",
        "catboost_best_grid = GridSearchCV(catboost_model, catboost_params, cv=5, n_jobs=-1, verbose=True).fit(X, y)\n",
        "catboost_final = catboost_model.set_params(**catboost_best_grid.best_params_, random_state=17).fit(X, y)\n",
        "rmse = np.mean(np.sqrt(-cross_val_score(catboost_final, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n",
        "rmse\n",
        "\n",
        "\n",
        "######################################################\n",
        "#  Automated Hyperparameter Optimization\n",
        "######################################################\n",
        "\n",
        "rf_params = {\"max_depth\": [5, 8, 15, None],\n",
        "             \"max_features\": [5, 7, \"auto\"],\n",
        "             \"min_samples_split\": [8, 15, 20],\n",
        "             \"n_estimators\": [200, 500]}\n",
        "\n",
        "gbm_params = {\"learning_rate\": [0.01, 0.1],\n",
        "              \"max_depth\": [3, 8],\n",
        "              \"n_estimators\": [500, 1000],\n",
        "              \"subsample\": [1, 0.5, 0.7]}\n",
        "\n",
        "\n",
        "lightgbm_params = {\"learning_rate\": [0.01, 0.1],\n",
        "                   \"n_estimators\": [300, 500],\n",
        "                   \"colsample_bytree\": [0.7, 1]}\n",
        "\n",
        "catboost_params = {\"iterations\": [200, 500],\n",
        "                   \"learning_rate\": [0.01, 0.1],\n",
        "                   \"depth\": [3, 6]}\n",
        "\n",
        "\n",
        "regressors = [(\"RF\", RandomForestRegressor(), rf_params),\n",
        "              ('GBM', GradientBoostingRegressor(), gbm_params),\n",
        "              ('LightGBM', LGBMRegressor(), lightgbm_params),\n",
        "              (\"CatBoost\", CatBoostRegressor(), catboost_params)]\n",
        "\n",
        "\n",
        "best_models = {}\n",
        "\n",
        "for name, regressor, params in regressors:\n",
        "    print(f\"########## {name} ##########\")\n",
        "    rmse = np.mean(np.sqrt(-cross_val_score(regressor, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n",
        "    print(f\"RMSE: {round(rmse, 4)} ({name}) \")\n",
        "\n",
        "    gs_best = GridSearchCV(regressor, params, cv=3, n_jobs=-1, verbose=False).fit(X, y)\n",
        "\n",
        "    final_model = regressor.set_params(**gs_best.best_params_)\n",
        "    rmse = np.mean(np.sqrt(-cross_val_score(final_model, X, y, cv=10, scoring=\"neg_mean_squared_error\")))\n",
        "    print(f\"RMSE (After): {round(rmse, 4)} ({name}) \")\n",
        "\n",
        "    print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n",
        "\n",
        "    best_models[name] = final_model\n",
        "\n",
        "\n",
        "################################################\n",
        "# Feature Importance\n",
        "################################################\n",
        "\n",
        "def plot_importance(model, features, num=len(X), save=False):\n",
        "    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    sns.set(font_scale=1)\n",
        "    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n",
        "                                                                     ascending=False)[0:num])\n",
        "    plt.title('Features')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    if save:\n",
        "        plt.savefig('importances.png')\n",
        "\n",
        "\n",
        "plot_importance(rf_final, X)\n",
        "plot_importance(gbm_final, X)\n",
        "plot_importance(lgbm_final, X)\n",
        "plot_importance(catboost_final, X)\n"
      ]
    }
  ]
}